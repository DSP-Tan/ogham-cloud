{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e6fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_scraper.doc_utils   import (open_exam, get_doc_line_df, identify_section_headers,\n",
    "                                     identify_text_headers,identify_footers, identify_instructions,\n",
    "                                     identify_subtitles, identify_subsubtitles,get_images,preproc_images,\n",
    "                                     assign_in_image_captions, identify_vertical_captions,\n",
    "                                     identify_all_page_clusters, enrich_doc_df_with_images)\n",
    "\n",
    "from pdf_scraper.line_utils    import clean_line_df, get_category_boxes, get_df_bbox\n",
    "from pdf_scraper.image_utils   import get_bboxed_page_image,show_image, show_all_imgs\n",
    "from pdf_scraper.general_utils import bbox_vert_dist, bbox_horiz_dist\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, io, re\n",
    "\n",
    "from IPython.display import display, clear_output, Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_df(doc):\n",
    "    df = get_doc_line_df(doc)\n",
    "    doc_width = doc[0].rect.width\n",
    "\n",
    "    images = preproc_images(get_images(doc))\n",
    "    assign_in_image_captions(df, images)\n",
    "\n",
    "    df = clean_line_df(df)\n",
    "    df = enrich_doc_df_with_images(df, images)\n",
    "\n",
    "    identify_all_page_clusters(df, 2.0/3.0, 1.15, text_only=True)\n",
    "    identify_footers(df)\n",
    "    identify_instructions(df)\n",
    "    identify_section_headers(df)\n",
    "    identify_text_headers(df, doc_width)\n",
    "    identify_subtitles(df, doc_width)\n",
    "    identify_subsubtitles(df, doc_width)\n",
    "\n",
    "    return df\n",
    "\n",
    "def view_year_page(year,page):\n",
    "    doc         = open_exam(year)\n",
    "    images      = preproc_images(get_images(doc))\n",
    "    df          = get_parsed_df(doc)\n",
    "\n",
    "    page_df = df[df.page == page]\n",
    "    rects   = get_category_boxes(page_df, \"cluster\")\n",
    "    img     = get_bboxed_page_image(doc, page, rects, labels = np.unique(page_df.cluster))\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae30532",
   "metadata": {},
   "source": [
    "# Check and clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"captioned_dfs.csv\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.sort_values(by=[\"year\",\"page\"],ignore_index=True,inplace=True)\n",
    "df.loc[df.category==\"caption2\",[\"text\",\"page\",\"year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81369642",
   "metadata": {},
   "source": [
    "The below check will identify cases where two labelling run throughs were made and different categorisations\n",
    "were assigned to the same lines in different run throughs. The code below will allow you to identify and then\n",
    "remove the incorrectly categorised lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd945c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "capt_df = df.loc[ df.category==\"caption2\", [\"text\",\"category\",\"year\",\"page\"]].copy()\n",
    "check = df[[\"text\",\"category\",\"year\",\"page\"]].merge(capt_df, \"inner\", on=[\"text\",\"year\",\"page\"])\n",
    "assert len(check[check.category_x != check.category_y]) == 0\n",
    "\n",
    "\n",
    "# Once you have identified incorrectly categorised lines using the code above, you can drop them using their\n",
    "# indices as below.\n",
    "# df.drop([12453,12454],inplace=True)\n",
    "# df.sort_values(by=[\"year\",\"page\"],ignore_index=True,inplace=True)\n",
    "# df.to_csv(\"captioned_dfs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3c6c3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb544ce",
   "metadata": {},
   "source": [
    "## Drop pages without images and unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2001, 2026):\n",
    "    print(year, end=\", \")\n",
    "    doc         = open_exam(year)\n",
    "    images      = preproc_images(get_images(doc))\n",
    "    image_pages = set(img[\"page\"] for img in images if img[\"page\"]>1)\n",
    "    drop_mask   = (df.year == year) & (~df.page.isin(image_pages))\n",
    "    to_drop     = df[drop_mask].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "\n",
    "df = df[[\"x0\",\"y0\",\"x1\",\"y1\", \"mode_font\",\"w\", \"h\", \"text\",\"font_size\",\"category\",\"page\",\"cluster\",\"year\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067e253",
   "metadata": {},
   "source": [
    "## Drop already categorised lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.category.isin( (\"uncategorised\",\"caption2\",\"image\") )\n",
    "print(len(df))\n",
    "df = df[mask]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bf4b1",
   "metadata": {},
   "source": [
    "## Convert line_df to block_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb94949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_func(text_series):\n",
    "    \"\"\"\n",
    "    Returns True if any text entry in the series likely refers to an image or figure caption.\n",
    "    Matches words and patterns like:\n",
    "      - 'image', 'image 1', 'Image:', etc.\n",
    "      - 'figure', 'figure 1', 'fig.', 'fig 2', etc.\n",
    "      - 'above', 'below', with or without colons or punctuation.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'\\b('\n",
    "        r'image\\s*\\d*[\\s:.,;)]*|'      # image, image 1:, image2, etc.\n",
    "        r'fig(?:ure)?\\s*\\d*[\\s:.,;)]*|' # fig, fig. 1, figure 2:, etc.\n",
    "        r'above[\\s:.,;)]*|'             # above, above:\n",
    "        r'below[\\s:.,;)]*'              # below, below:\n",
    "        r')',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return text_series.astype(str).apply(lambda t: bool(pattern.search(t))).any()\n",
    "\n",
    "agg_dict = {\n",
    "    \"x0\": \"min\",\n",
    "    \"y0\": \"min\",\n",
    "    \"x1\": \"max\",\n",
    "    \"y1\": \"max\",\n",
    "    \"mode_font\": lambda x: x.mode().iat[0] if not x.mode().empty else None,\n",
    "    \"w\": \"median\",\n",
    "    \"h\": \"median\",\n",
    "    \"text\": lambda x: \"\\n\".join(x.astype(str)),\n",
    "    #\"text\": text_func,\n",
    "    \"font_size\": \"median\",\n",
    "    \"category\": lambda x: x.mode().iat[0] if not x.mode().empty else None\n",
    "}\n",
    "\n",
    "new_col_aggs = {\n",
    "    \"n_lines\": (\"x0\", \"count\")\n",
    "}\n",
    "\n",
    "block_df            = df.groupby([\"year\",\"page\",\"cluster\"]).agg(agg_dict).reset_index()\n",
    "block_df[\"n_lines\"] = df.groupby([\"year\",\"page\",\"cluster\"]).agg(n_lines=(\"x0\",\"count\")).values\n",
    "\n",
    "\n",
    "text_block_df  = block_df[block_df.category !=\"image\"].copy()\n",
    "image_block_df = block_df[block_df.category ==\"image\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613978d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df.loc[(block_df.category==\"image\"), \"h\"]  = block_df[block_df.category==\"image\"].y1 - block_df.loc[block_df.category==\"image\"].y0\n",
    "block_df.loc[(block_df.category==\"image\"), \"w\"]  = block_df[block_df.category==\"image\"].x1 - block_df.loc[block_df.category==\"image\"].x0\n",
    "block_df[block_df.category==\"image\"].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377077f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "capt_block_df = block_df[block_df.category==\"caption2\"].copy()\n",
    "print(f'Data is {100*len(capt_block_df)/len(block_df[block_df.category !=\"image\"]):0.2f}% caption.')\n",
    "capt_block_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a41e9",
   "metadata": {},
   "source": [
    "## Find nearest image to block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each block, check the distance to every image on the same page and year,\n",
    "# choose block which is closest. dx will have to be 0 for it to be a vertical caption.\n",
    "# - filter only blocks with dx == 0\n",
    "# - amongst those return the lowest dy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1be46",
   "metadata": {},
   "source": [
    "### Via images dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_overlapping_with_bbox(bbox, page, images):\n",
    "    \"\"\"\n",
    "    Note: These images must be from the same year as the bbox.\n",
    "    \"\"\"\n",
    "    imgs = [img for img in images if img[\"page\"]==page]\n",
    "    ov_ims = []\n",
    "    for img in imgs:\n",
    "        dx = bbox_horiz_dist(bbox, img[\"bbox\"])\n",
    "        if dx == 0:\n",
    "            print(f\"overlaps with image: {img[\"number\"]}\")\n",
    "            ov_ims.append(img)\n",
    "    return ov_ims\n",
    "\n",
    "def nearest_overlapping_image(bbox, imgs):\n",
    "    \"\"\"\n",
    "    Of those images which overlap in the x direction, which is the nearest vertically?\n",
    "    \"\"\"\n",
    "    dy_min = 1000.0\n",
    "    nearest_image = None\n",
    "    for im in imgs:\n",
    "        im_bbox = im[\"bbox\"]\n",
    "        dy = bbox_vert_dist(bbox,im_bbox)\n",
    "        if  dy < dy_min:\n",
    "            nearest_image = im\n",
    "            dy_min = dy\n",
    "    return (nearest_image, dy_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e70d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "year= 2011\n",
    "doc = open_exam(year)\n",
    "images = preproc_images(get_images(doc))\n",
    "capt_bbox = get_df_bbox( capt_block_df.loc[[177]])\n",
    "page = capt_block_df.loc[177].page\n",
    "imgs = images_overlapping_with_bbox(capt_bbox, page, images)\n",
    "\n",
    "show_all_imgs(1,3, imgs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year= 2011\n",
    "doc = open_exam(year)\n",
    "images = preproc_images(get_images(doc))\n",
    "capt_bbox = get_df_bbox( capt_block_df.loc[[177]])\n",
    "page = capt_block_df.loc[177].page\n",
    "imgs = images_overlapping_with_bbox(capt_bbox, page, images)\n",
    "nearest_img, dy = nearest_overlapping_image(capt_bbox, imgs)\n",
    "\n",
    "#show_image(img )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e0afd",
   "metadata": {},
   "source": [
    "### Via image-enriched dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_image_indices(row, block_df):\n",
    "    \"\"\"\n",
    "    Note: These images must be from the same year as the bbox.\n",
    "    \"\"\"\n",
    "    row_year  = (block_df.year == row.year)\n",
    "    row_page  = (block_df.page == row.page)\n",
    "    is_image  = (block_df.category==\"image\")\n",
    "\n",
    "    bbox   = row.x0, row.y0, row.x1, row.y1\n",
    "    img_df = block_df[ is_image & row_page & row_year ].copy()\n",
    "\n",
    "    img_df[\"dx\"] = img_df.apply( lambda x: bbox_horiz_dist(bbox, (x.x0, x.y0, x.x1, x.y1) ) , axis=1)\n",
    "    indices = img_df[img_df.dx==0].index\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = block_df.loc[227]\n",
    "year   = row.year\n",
    "page   = row.page\n",
    "bbox   = row.x0, row.y0, row.x1, row.y1\n",
    "img_df = block_df[(block_df.category==\"image\") & (block_df.year==year) & (block_df.page==page)].copy()\n",
    "\n",
    "img_df[\"dx\"] =img_df.apply( lambda x: bbox_horiz_dist(bbox, (x.x0, x.y0, x.x1, x.y1) ), axis=1)\n",
    "indices = img_df[img_df.dx==0].index\n",
    "indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f514e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_block_df[\"im_ovs\"]=text_block_df.apply(lambda x: get_overlapping_image_indices(x, block_df).shape[0], axis=1)\n",
    "\n",
    "all_ovs = text_block_df.apply(lambda x: get_overlapping_image_indices(x, block_df), axis=1)\n",
    "text_block_df[\"img_ov\"] = all_ovs.apply( lambda x: True if len(x)>0 else False)\n",
    "text_block_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a771dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_block_df[text_block_df.img_ov]))\n",
    "print(len(text_block_df))\n",
    "print(f'Data is {100*len(capt_block_df)/len(text_block_df[text_block_df.img_ov]):0.2f}% caption.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_image_index(row, block_df):\n",
    "    bbox1   = row.x0, row.y0, row.x1, row.y1\n",
    "    i_img   = get_overlapping_image_indices(row,block_df)\n",
    "    img_df  = block_df.loc[i_img]\n",
    "\n",
    "    dy = img_df.apply( lambda x: bbox_vert_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) ), axis=1 )\n",
    "\n",
    "    if len(dy) >0:\n",
    "        return dy.idxmin()\n",
    "    return np.nan\n",
    "\n",
    "def get_nearest_image_distance(row , block_df):\n",
    "    bbox1 = row.x0, row.y0, row.x1, row.y1\n",
    "\n",
    "    i_image = get_nearest_image_index(row, block_df)\n",
    "    if np.isnan(i_image):\n",
    "        return np.nan\n",
    "    im_row = block_df.loc[i_image]\n",
    "    bbox2 = im_row.x0, im_row.y0, im_row.x1, im_row.y1\n",
    "\n",
    "    dy = bbox_vert_dist(bbox1, bbox2 )\n",
    "\n",
    "    return  dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c492d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = capt_block_df.loc[227]\n",
    "im_idx = get_nearest_image_index(row, block_df)\n",
    "image_block_df.loc[im_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nearest_image_distance(row, block_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_block_df.apply(lambda x: get_nearest_image_distance(x, block_df), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa66902",
   "metadata": {},
   "source": [
    "## Define block properties relative to nearest image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab582fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_image_relative_width(row , block_df):\n",
    "\n",
    "    width_txt = row.w\n",
    "\n",
    "    i_image = get_nearest_image_index(row, block_df)\n",
    "    if np.isnan(i_image):\n",
    "        return np.nan\n",
    "    width_im = block_df.loc[i_image].w\n",
    "\n",
    "    return width_txt/ width_im\n",
    "\n",
    "text_block_df.apply(lambda x: get_nearest_image_relative_width(x, block_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48076dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_image_centre_offset(row , block_df):\n",
    "\n",
    "    row_centre = (row.x0 + row.x1)/2\n",
    "\n",
    "    i_image = get_nearest_image_index(row, block_df)\n",
    "    if np.isnan(i_image):\n",
    "        return np.nan\n",
    "    im_row = block_df.loc[i_image]\n",
    "    im_centre = (im_row.x0 + im_row.x1)/2\n",
    "\n",
    "    return abs(im_centre - row_centre)\n",
    "\n",
    "text_block_df.apply(lambda x: get_nearest_image_centre_offset(x, block_df), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca8fc9",
   "metadata": {},
   "source": [
    "## Distance to nearest text block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_to_nearest_text_block(row, block_df):\n",
    "    bbox1        = row.x0, row.y0, row.x1, row.y1\n",
    "\n",
    "    exclude_self = (block_df.index != row.name)\n",
    "    row_year     = (block_df.year == row.year)\n",
    "    row_page     = (block_df.page == row.page)\n",
    "    not_image    = (block_df.category != \"image\")\n",
    "\n",
    "    text_df      = block_df.loc[ exclude_self & not_image & row_year & row_page ]\n",
    "    overlapping  = text_df.apply(lambda x: bbox_horiz_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) )== 0 ,axis=1 )\n",
    "\n",
    "    ov_text_df   = text_df[overlapping]\n",
    "\n",
    "    dy = ov_text_df.apply( lambda x: bbox_vert_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) ), axis=1 )\n",
    "\n",
    "    if len(dy) >0:\n",
    "        return dy.min()\n",
    "    return np.nan\n",
    "\n",
    "text_block_df.apply(lambda x: get_distance_to_nearest_text_block(x, block_df), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogham",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
