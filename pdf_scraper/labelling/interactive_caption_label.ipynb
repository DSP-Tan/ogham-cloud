{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_scraper.doc_utils   import (open_exam, get_doc_line_df, identify_section_headers, \n",
    "                                     identify_text_headers,identify_footers, identify_instructions, \n",
    "                                     identify_subtitles, identify_subsubtitles,get_images,preproc_images, \n",
    "                                     assign_in_image_captions, identify_vertical_captions,\n",
    "                                     identify_all_page_clusters, enrich_doc_df_with_images)\n",
    "\n",
    "from pdf_scraper.line_utils    import clean_line_df, get_category_boxes, get_df_bbox\n",
    "from pdf_scraper.image_utils   import get_bboxed_page_image,show_image, show_all_imgs\n",
    "from pdf_scraper.general_utils import bbox_vert_dist, bbox_horiz_dist\n",
    "from time import sleep\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import os, io, re\n",
    "\n",
    "from IPython.display import display, clear_output, Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307dd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_df(doc):\n",
    "    df = get_doc_line_df(doc)\n",
    "    doc_width = doc[0].rect.width\n",
    "\n",
    "    images = preproc_images(get_images(doc))\n",
    "    assign_in_image_captions(df, images)\n",
    "\n",
    "    df = clean_line_df(df)\n",
    "    df = enrich_doc_df_with_images(df, images)\n",
    "\n",
    "    identify_all_page_clusters(df, 2.0/3.0, 1.15, text_only=True)\n",
    "    identify_footers(df)\n",
    "    identify_instructions(df)\n",
    "    identify_section_headers(df)\n",
    "    identify_text_headers(df, doc_width)\n",
    "    identify_subtitles(df, doc_width)\n",
    "    identify_subsubtitles(df, doc_width)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_input(df, page):\n",
    "    while True:\n",
    "       caption_input = input(\"Caption text: \").strip().lower()\n",
    "       if caption_input == \"\":\n",
    "           break\n",
    "\n",
    "       matches = df[(df.page == page) & (df.text.str.strip().str.lower().str.contains(caption_input)) ]\n",
    "       if matches.empty:\n",
    "           print(\"No match found. Retype, perhaps with more of line.\")\n",
    "           continue\n",
    "       elif len(matches) > 1: \n",
    "           print(f\"{len(matches)} matches:\")\n",
    "           display(matches.head(10))\n",
    "           index_input = input(\"Select index: \")\n",
    "           index_input = int(index_input)\n",
    "           matches     = df.loc[[index_input]]\n",
    "\n",
    "       df.loc[matches.index, \"category\"] = \"caption2\"\n",
    "       print(f\"Marked: {matches.text.values[0]} as caption2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image(image_handle, img):\n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(img); plt.axis(\"off\"); plt.tight_layout()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "\n",
    "    image_handle.update(Image(data=buf.read()))\n",
    "    return image_handle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "output_csv = \"captioned_dfs.csv\"\n",
    "\n",
    "for year in range(2020, 2021):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"\\n----- Year: {year} -----\\n\")\n",
    "    doc         = open_exam(year)\n",
    "    images      = preproc_images(get_images(doc))\n",
    "    df          = get_parsed_df(doc)\n",
    "    df[\"year\"]  = year\n",
    "\n",
    "    image_handle = display(None, display_id=True)\n",
    "    if year in [y for y in range(2001, 2011)]:\n",
    "        file_exists = os.path.exists(output_csv)\n",
    "        df.to_csv(output_csv, mode=\"a\", header=not file_exists, index=False)\n",
    "        dfs.append(df)\n",
    "        continue\n",
    "        \n",
    "    image_pages = set(img[\"page\"] for img in images if img[\"page\"]>1)\n",
    "    for page in sorted(image_pages):\n",
    "        page_df = df[df.page == page]\n",
    "        rects   = get_category_boxes(page_df, \"cluster\")\n",
    "        img     = get_bboxed_page_image(doc, page, rects, labels = np.unique(page_df.cluster))\n",
    "        \n",
    "        image_handle = update_image(image_handle, img)\n",
    "        print(f\"\\n{year} --- Page {page} ---\")\n",
    "        \n",
    "        has_captions = input(\"Are there captions on this page? (y/n): \").strip().lower()\n",
    "        if has_captions not in (\"y\", \"yes\"):\n",
    "            print(\"Skipping this page.\\n\")\n",
    "            continue\n",
    "        get_caption_input(df, page)\n",
    "\n",
    "        display(df.loc[(df.category==\"caption2\") & (df.page==page),[\"text\",\"page\"] ].head())\n",
    "        good_page = input(\"Are these captions correct? (y/n): \").strip().lower()\n",
    "        if good_page not in (\"y\", \"yes\"):\n",
    "            df.loc[(df.page==page) & (df.category==\"caption2\"), \"category\"] = \"uncategorised\"\n",
    "            get_caption_input(df, page)\n",
    "        print(f\"Done with page {page}.\\n\")\n",
    "    \n",
    "    file_exists = os.path.exists(output_csv)\n",
    "    df.to_csv(output_csv, mode=\"a\", header=not file_exists, index=False)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nFinal dataframe shape:\", big_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c442e2e",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22c455",
   "metadata": {},
   "source": [
    "## Check and clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23712efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"captioned_dfs.csv\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.sort_values(by=[\"year\",\"page\"],ignore_index=True,inplace=True)\n",
    "df.loc[df.category==\"caption2\",[\"text\",\"page\",\"year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71669b7",
   "metadata": {},
   "source": [
    "The below check will identify cases where two labelling run throughs were made and different categorisations\n",
    "were assigned to the same lines in different run throughs. The code below will allow you to identify and then\n",
    "remove the incorrectly categorised lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38252927",
   "metadata": {},
   "outputs": [],
   "source": [
    "capt_df = df.loc[ df.category==\"caption2\", [\"text\",\"category\",\"year\",\"page\"]].copy()\n",
    "check = df[[\"text\",\"category\",\"year\",\"page\"]].merge(capt_df, \"inner\", on=[\"text\",\"year\",\"page\"])\n",
    "assert len(check[check.category_x != check.category_y]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have identified incorrectly categorised lines using the code above, you can drop them using their\n",
    "# indices as below.\n",
    "# df.drop([12453,12454],inplace=True)\n",
    "# df.sort_values(by=[\"year\",\"page\"],ignore_index=True,inplace=True)\n",
    "# df.to_csv(\"captioned_dfs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec58df",
   "metadata": {},
   "source": [
    "## Enrich for caption detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09538f5",
   "metadata": {},
   "source": [
    "### Drop pages without images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc625ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2001, 2026):\n",
    "    print(year, end=\", \")\n",
    "    doc         = open_exam(year)\n",
    "    images      = preproc_images(get_images(doc))\n",
    "    image_pages = set(img[\"page\"] for img in images if img[\"page\"]>1)\n",
    "    drop_mask   = (df.year == year) & (~df.page.isin(image_pages)) \n",
    "    to_drop     = df[drop_mask].index\n",
    "    df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7329ef",
   "metadata": {},
   "source": [
    "### Drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"x0\",\"y0\",\"x1\",\"y1\", \"mode_font\",\"w\", \"h\", \"text\",\"font_size\",\"category\",\"page\",\"cluster\",\"year\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb100b95",
   "metadata": {},
   "source": [
    "### Drop already categorised lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc52758",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.category.isin( (\"uncategorised\",\"caption2\",\"image\") ) \n",
    "print(len(df))\n",
    "df = df[mask]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152b626",
   "metadata": {},
   "source": [
    "### Is line within image bound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.category==\"caption2\", [\"text\",\"category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c1662",
   "metadata": {},
   "source": [
    "### Convert line_df to block_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_func(text_series):\n",
    "    \"\"\"\n",
    "    Returns True if any text entry in the series likely refers to an image or figure caption.\n",
    "    Matches words and patterns like:\n",
    "      - 'image', 'image 1', 'Image:', etc.\n",
    "      - 'figure', 'figure 1', 'fig.', 'fig 2', etc.\n",
    "      - 'above', 'below', with or without colons or punctuation.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'\\b('\n",
    "        r'image\\s*\\d*[\\s:.,;)]*|'      # image, image 1:, image2, etc.\n",
    "        r'fig(?:ure)?\\s*\\d*[\\s:.,;)]*|' # fig, fig. 1, figure 2:, etc.\n",
    "        r'above[\\s:.,;)]*|'             # above, above:\n",
    "        r'below[\\s:.,;)]*'              # below, below:\n",
    "        r')',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return text_series.astype(str).apply(lambda t: bool(pattern.search(t))).any()\n",
    "\n",
    "agg_dict = {\n",
    "    \"x0\": \"min\",\n",
    "    \"y0\": \"min\",\n",
    "    \"x1\": \"max\",\n",
    "    \"y1\": \"max\",\n",
    "    \"mode_font\": lambda x: x.mode().iat[0] if not x.mode().empty else None,\n",
    "    \"w\": \"median\",\n",
    "    \"h\": \"median\",\n",
    "    \"text\": lambda x: \"\\n\".join(x.astype(str)),\n",
    "    #\"text\": text_func,\n",
    "    \"font_size\": \"median\",\n",
    "    \"category\": lambda x: x.mode().iat[0] if not x.mode().empty else None,\n",
    "}\n",
    "\n",
    "block_df =df.groupby([\"year\",\"page\",\"cluster\"]).agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df.loc[(block_df.category==\"image\"), \"h\"]  = block_df[block_df.category==\"image\"].y1 - block_df.loc[block_df.category==\"image\"].y0\n",
    "block_df.loc[(block_df.category==\"image\"), \"w\"]  = block_df[block_df.category==\"image\"].x1 - block_df.loc[block_df.category==\"image\"].x0\n",
    "block_df[block_df.category==\"image\"].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3700da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "capt_block_df = block_df[block_df.category==\"caption2\"].copy()\n",
    "print(f'Data is {100*len(capt_block_df)/len(block_df[block_df.category !=\"image\"]):0.2f}% caption.')\n",
    "capt_block_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41cfb5",
   "metadata": {},
   "source": [
    "### Find nearest image to block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each block, check the distance to every image on the same page and year, \n",
    "# choose block which is closest. dx will have to be 0 for it to be a vertical caption.\n",
    "# - filter only blocks with dx == 0\n",
    "# - amongst those return the lowest dy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a792826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_overlapping_with_bbox(bbox, page, images):\n",
    "    \"\"\"\n",
    "    Note: These images must be from the same year as the bbox.\n",
    "    \"\"\"\n",
    "    imgs = [img for img in images if img[\"page\"]==page]\n",
    "    ov_ims = []\n",
    "    for img in imgs:\n",
    "        dx = bbox_horiz_dist(bbox, img[\"bbox\"])\n",
    "        if dx == 0:\n",
    "            print(f\"overlaps with image: {img[\"number\"]}\")\n",
    "            ov_ims.append(img)\n",
    "    return ov_ims\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = block_df.loc[227]\n",
    "year   = row.year\n",
    "page   = row.page\n",
    "bbox   = row.x0, row.y0, row.x1, row.y1\n",
    "img_df = block_df[(block_df.category==\"image\") & (block_df.year==year) & (block_df.page==page)].copy()\n",
    "\n",
    "img_df[\"dx\"] =img_df.apply( lambda x: bbox_horiz_dist(bbox, (x.x0, x.y0, x.x1, x.y1) ), axis=1)\n",
    "img_df[img_df.dx==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_images(row, block_df):\n",
    "    \"\"\"\n",
    "    Note: These images must be from the same year as the bbox.\n",
    "    \"\"\"\n",
    "    year   = row.year\n",
    "    page   = row.page\n",
    "    bbox   = row.x0, row.y0, row.x1, row.y1\n",
    "    img_df = block_df[(block_df.category==\"image\") & (block_df.year==year) & (block_df.page==page)].copy()\n",
    "\n",
    "    img_df[\"dx\"] = img_df.apply( lambda x: bbox_horiz_dist(bbox, (x.x0, x.y0, x.x1, x.y1) ) , axis=1)\n",
    "    indices = img_df[img_df.dx==0].index\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f408f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = block_df.loc[227]\n",
    "\n",
    "indices = images_overlapping_with_bbox(row, block_df)\n",
    "img_df = block_df.loc[indices]\n",
    "\n",
    "\n",
    "bbox1 = row.x0, row.y0, row.x1, row.y1\n",
    "dy = img_df.apply( lambda x: bbox_vert_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) ), axis=1 )   \n",
    "\n",
    "dy_min     = dy.min()\n",
    "nearest_im = img_df.loc[dy.idxmin()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3849d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_image(row, img_df):\n",
    "    bbox1 = row.x0, row.y0, row.x1, row.y1\n",
    "    dy = img_df.apply( lambda x: bbox_vert_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) ), axis=1 )   \n",
    "    \n",
    "    nearest_im = img_df.loc[dy.idxmin()]\n",
    "    return nearest_im\n",
    "\n",
    "def get_nearest_image_width(row, img_df):\n",
    "    im =get_nearest_image(row,img_df)\n",
    "    return im.w\n",
    "\n",
    "def get_nearest_image_height(row, img_df):\n",
    "    im =get_nearest_image(row,img_df)\n",
    "    return im.h\n",
    "\n",
    "def get_nearest_image_centre(row, img_df):\n",
    "    im =get_nearest_image(row,img_df)\n",
    "    return (im.x0 + im.x1)/2\n",
    "\n",
    "def get_nearest_image_width(row, img_df):\n",
    "    im =get_nearest_image(row,img_df)\n",
    "    return im.h\n",
    "\n",
    "def get_nearest_image_distance(row , img_df):\n",
    "    bbox1 = row.x0, row.y0, row.x1, row.y1\n",
    "    dy = img_df.apply( lambda x: bbox_vert_dist(bbox1, (x.x0,x.y0,x.x1,x.y1) ), axis=1 )   \n",
    "    \n",
    "    return  dy.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year= 2011\n",
    "doc = open_exam(year)\n",
    "images = preproc_images(get_images(doc))\n",
    "capt_bbox = get_df_bbox( capt_block_df.loc[[177]])\n",
    "page = capt_block_df.loc[177].page\n",
    "imgs = images_overlapping_with_bbox(capt_bbox, page, images)\n",
    "\n",
    "show_all_imgs(1,3, imgs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_overlapping_image(bbox, imgs):\n",
    "    \"\"\"\n",
    "    Of those images which overlap in the x direction, which is the nearest vertically?\n",
    "    \"\"\"\n",
    "    dy_min = 1000.0\n",
    "    nearest_image = None\n",
    "    for im in imgs:\n",
    "        im_bbox = im[\"bbox\"]\n",
    "        dy = bbox_vert_dist(bbox,im_bbox)\n",
    "        if  dy < dy_min:\n",
    "            nearest_image = im\n",
    "            dy_min = dy\n",
    "    return (nearest_image, dy_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "year= 2011\n",
    "doc = open_exam(year)\n",
    "images = preproc_images(get_images(doc))\n",
    "capt_bbox = get_df_bbox( capt_block_df.loc[[177]])\n",
    "page = capt_block_df.loc[177].page\n",
    "imgs = images_overlapping_with_bbox(capt_bbox, page, images)\n",
    "nearest_img, dy = nearest_overlapping_image(capt_bbox, imgs)\n",
    "\n",
    "#show_image(img )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = capt_block_df.loc[177]\n",
    "row.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf940b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dy_to_nearest_image(row):\n",
    "    year = row.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75c020",
   "metadata": {},
   "source": [
    "### Is block aligned in x with image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40df8ec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogham",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
